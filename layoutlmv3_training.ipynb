{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1amhcYGvDccY88d8zwsUf6oRGQy9JspDK","authorship_tag":"ABX9TyMpQkX3ALqC+G3+lccoPLde"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# LayoutLMv3 Training for SRE\n","\n","In this notebook, we will try to train LayoutLMv3 for custom SER task dataset. Specifically, we will train the model using the dataset that you have labeled using Label Studio"],"metadata":{"id":"oQQB8tKFsAEg"}},{"cell_type":"markdown","source":["## Dataset Preparation\n","\n","First, prepare the dataset. Copy the annotation results from the label studio like below."],"metadata":{"id":"H3lbwyCrtHiR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Ea9HL2xRn_v"},"outputs":[],"source":["!cp -r \"/content/drive/MyDrive/Public/Dibimbing/25 - OCR/Assignment/handwriting\" \"./handwriting\""]},{"cell_type":"code","source":["!pip install accelerate datasets transformers seqeval"],"metadata":{"id":"IMZZaTpxR0pp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","from pathlib import Path\n","from typing import List\n","\n","from torch.utils.data import Dataset\n","from transformers import AutoProcessor\n","from PIL import Image\n","\n","LBL2ID = {\n","    \"O\": 0,\n","    \"NUM\": 1,\n","    \"B-QUESTION\": 2,\n","    \"I-QUESTION\": 3,\n","    \"B-ANSWER\": 4,\n","    \"I-ANSWER\": 5,\n","}\n","ID2LBL = {v: k for k, v in LBL2ID.items()}\n","LABEL_LIST = list(LBL2ID)"],"metadata":{"id":"CslCYcrRUCdH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_image_name(ls_image_path: Path) -> str:\n","    \"\"\"\n","    Label studio will write the image file name in format of\n","    '{random_id}-{original_image_name}'. So we only want to\n","    get the original image name, since that is the name that\n","    we have.\n","    \"\"\"\n","    name = ls_image_path.name\n","    name = name[(name.find(\"-\") + 1):]\n","    return name\n","\n","def load_annotation_json(annotation, train_dir, val_dir):\n","    with open(annotation, \"r\") as f:\n","        data_raw = json.load(f)\n","    for d in data_raw:\n","        d[\"ocr\"] = get_image_name(Path(d[\"ocr\"]))\n","    train_images = [p.name for p in train_dir.glob(\"*\")]\n","    val_images = [p.name for p in val_dir.glob(\"*\")]\n","    train_anno = [ann for ann in data_raw if ann[\"ocr\"] in train_images]\n","    val_anno = [ann for ann in data_raw if ann[\"ocr\"] in val_images]\n","    return train_anno, val_anno\n","\n","def xywh2xyxy(xywh: List[float], img_width: int, img_height: int) -> List[int]:\n","    \"\"\"\n","    Change bounding box format xywh normalized 0-100 to\n","    xyxy normalized 0-1000.\n","    \"\"\"\n","    x, y, w, h = xywh\n","    x = x * 10\n","    y = y * 10\n","    w = w * 10\n","    h = h * 10\n","    return [\n","        int(x),\n","        int(y),\n","        int(x + w),\n","        int(y + h),\n","    ]\n","\n","def extract_box(ls_box):\n","    xywh = [ls_box[\"x\"], ls_box[\"y\"], ls_box[\"width\"], ls_box[\"height\"]]\n","    return xywh2xyxy(xywh, ls_box[\"original_width\"], ls_box[\"original_height\"])\n","\n","def extract_annotation_data(annotation, images_dir):\n","    converted_ann  = {}\n","    converted_ann[\"img_path\"] = images_dir / annotation[\"ocr\"]\n","    converted_ann[\"boxes\"] = [extract_box(lbl) for lbl in annotation[\"label\"]]\n","    converted_ann[\"words\"] = annotation[\"transcription\"]\n","    converted_ann[\"word_labels\"] = [LBL2ID[lbl[\"labels\"][0]] for lbl in annotation[\"label\"]]\n","    return converted_ann\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, annotations, processor):\n","        self.annotations = annotations\n","        self.processor = processor\n","        self.dataset = [self._create_feature(ann) for ann in self.annotations]\n","\n","    def _create_feature(self, ann):\n","        image = Image.open(ann[\"img_path\"]).convert('RGB')\n","        encoding = self.processor(\n","            image,\n","            ann[\"words\"],\n","            boxes=ann[\"boxes\"],\n","            word_labels=ann[\"word_labels\"],\n","            return_tensors=\"pt\",\n","            truncation=True,\n","            padding=\"max_length\",\n","        )\n","        encoding = {k:v.squeeze() for k,v in encoding.items()}\n","        return encoding\n","\n","    def __getitem__(self, idx):\n","        return self.dataset[idx]\n","\n","    def __len__(self):\n","        return len(self.dataset)"],"metadata":{"id":"XD6of_spSs3L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ann_file = Path(\"/content/handwriting/label-studio-anno.json\")\n","train_dir = Path(\"/content/handwriting/training\")\n","val_dir = Path(\"/content/handwriting/test\")\n","\n","train_anns_ls, val_anns_ls =  load_annotation_json(ann_file, train_dir, val_dir)\n","train_anns = [extract_annotation_data(ann, train_dir) for ann in train_anns_ls]\n","val_anns = [extract_annotation_data(ann, val_dir) for ann in val_anns_ls]\n","\n","processor = AutoProcessor.from_pretrained(\n","    \"microsoft/layoutlmv3-base\",\n","    apply_ocr=False,\n",")\n","train_dataset = CustomDataset(train_anns, processor)\n","val_dataset = CustomDataset(val_anns, processor)\n","combined_dataset = CustomDataset([*train_anns, *val_anns], processor)"],"metadata":{"id":"Ah2HhyMEWD9C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset[0].keys()"],"metadata":{"id":"0-w4DOJAKOuG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for id, label in zip(val_dataset[0][\"input_ids\"], val_dataset[0][\"labels\"]):\n","  print(processor.tokenizer.decode([id]), label.item())"],"metadata":{"id":"dqGz1ygPUwkA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"vnjR6IcqBaDB"}},{"cell_type":"code","source":["from datasets import load_metric\n","\n","metric = load_metric(\"seqeval\")"],"metadata":{"id":"vLUoMoEo_EpF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","return_entity_level_metrics = False\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [LABEL_LIST[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [LABEL_LIST[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    if return_entity_level_metrics:\n","        # Unpack nested dictionaries\n","        final_results = {}\n","        for key, value in results.items():\n","            if isinstance(value, dict):\n","                for n, v in value.items():\n","                    final_results[f\"{key}_{n}\"] = v\n","            else:\n","                final_results[key] = value\n","        return final_results\n","    else:\n","        return {\n","            \"precision\": results[\"overall_precision\"],\n","            \"recall\": results[\"overall_recall\"],\n","            \"f1\": results[\"overall_f1\"],\n","            \"accuracy\": results[\"overall_accuracy\"],\n","        }"],"metadata":{"id":"vL9PknaRCH0R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import LayoutLMv3ForTokenClassification, TrainingArguments, Trainer\n","from transformers.data.data_collator import default_data_collator\n","\n","model = LayoutLMv3ForTokenClassification.from_pretrained(\n","    \"microsoft/layoutlmv3-base\",\n","    id2label=ID2LBL,\n","    label2id=LBL2ID,\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"test\",\n","    max_steps=100,\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    learning_rate=1e-5,\n","    evaluation_strategy=\"steps\",\n","    eval_steps=10,\n","    metric_for_best_model=\"f1\",\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=processor,\n","    data_collator=default_data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"],"metadata":{"id":"CzqVfnOGDyrI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"a61sZjEDEiET"},"execution_count":null,"outputs":[]}]}